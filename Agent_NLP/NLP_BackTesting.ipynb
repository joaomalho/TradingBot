{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __NLP Yahoo Economic Calendar__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='white'> __1.1 Libraries__ </font> <a class=\"anchor\" id=\"oneone\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 18, 2]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install wordcloud\n",
    "#!pip install langdetect\n",
    "#!pip install googletrans\n",
    "#!pip install translate\n",
    "#!pip install emoji == 1.7\n",
    "#!pip install torchtext\n",
    "\n",
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import plotly.express as px\n",
    "from langdetect import detect\n",
    "import torchtext.vocab as vocab\n",
    "import matplotlib.pyplot as plt\n",
    "from translate import Translator\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.corpus import stopwords\n",
    "import plotly.graph_objects as go\n",
    "from googletrans import Translator\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "driver.get(url='https://pt.investing.com/economic-calendar/')\n",
    "\n",
    "def xpath_click(path):\n",
    "    try:\n",
    "        time.sleep(.6)\n",
    "        driver.implicitly_wait(7)\n",
    "        r = driver.find_elements(By.XPATH, path)\n",
    "        r.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Set Chrome to run in headless mode\n",
    "# chrome_options = webdriver.ChromeOptions()\n",
    "# chrome_options.add_argument('--headless')\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "# driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "driver.get(url='https://pt.investing.com/economic-calendar')\n",
    "\n",
    "def xpath_click(path):\n",
    "    time.sleep(.6)\n",
    "    driver.implicitly_wait(7)\n",
    "    r = driver.find_element(By.XPATH, path)\n",
    "    r.click()\n",
    "\n",
    "def xpath_clicker(path):\n",
    "    try:\n",
    "        xpath_click(path)\n",
    "        try:\n",
    "            xpath_click(path)\n",
    "        except:\n",
    "            xpath_click(path)\n",
    "    except:\n",
    "        return KeyError\n",
    "\n",
    "# Click any buttons or perform actions to load the table if necessary\n",
    "xpath_clicker('/html/body/div[10]/div[2]/div/div/div[2]/div/div/button')\n",
    "\n",
    "table_xpath = \"/html/body/div[6]/section/div[6]/table\"\n",
    "table = driver.find_element(By.XPATH, table_xpath)\n",
    "\n",
    "# Get the innerHTML of the table\n",
    "table_html = table.get_attribute(\"innerHTML\")\n",
    "\n",
    "# Parse the table HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(table_html, 'html.parser')\n",
    "\n",
    "# Extract the table headers\n",
    "headers = [header.text for header in soup.find_all('th')]\n",
    "\n",
    "# Initialize an empty list to store the table data\n",
    "table_data = []\n",
    "\n",
    "# Extract table rows\n",
    "for row in soup.find_all('tr'):\n",
    "    # Extract row data\n",
    "    row_data = [cell.text for cell in row.find_all('td')]\n",
    "    table_data\n",
    "\n",
    "table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Hour Ticker Imp                                              Event  \\\n",
      "0   00:01    GBP      Volume de Negócios no Comércio a Retalho BRC (...   \n",
      "1   03:00    CNY                                      Exportações (Out)   \n",
      "2   03:00    CNY                                      Importações (Out)   \n",
      "3   03:00    CNY                                Balança Comercial (Out)   \n",
      "4   03:00    CNY                              Exportações (Anual) (Out)   \n",
      "..    ...    ...  ..                                                ...   \n",
      "58  19:30    EUR            Discurso de Nagel, Presidente do Bundesbank   \n",
      "59  20:00    USD                            Crédito no Consumidor (Set)   \n",
      "60  21:30    USD               Inventário de Petróleo Bruto Semanal API   \n",
      "61  23:00    JPY                            Índice Tankan Reuters (Nov)   \n",
      "62  23:50    JPY                    Reservas Internacionais (USD) (Out)   \n",
      "\n",
      "     Actual Forecast  Previous  \n",
      "0      2,6%     2,4%      2,8%  \n",
      "1    -3,10M             -0,60M  \n",
      "2     6,40M             -0,80M  \n",
      "3   405,47B  572,00B   558,74B  \n",
      "4     -6,4%    -3,3%     -6,2%  \n",
      "..      ...      ...       ...  \n",
      "58                              \n",
      "59            10,00B   -15,63B  \n",
      "60                      1,347M  \n",
      "61                           4  \n",
      "62                    1.237,2B  \n",
      "\n",
      "[63 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the website you want to scrape\n",
    "url = \"https://pt.investing.com/economic-calendar/\"\n",
    "\n",
    "# Send an HTTP GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find the table you want to scrape based on its class or other attributes\n",
    "    table = soup.find(\"table\", {\"id\": \"economicCalendarData\"})\n",
    "\n",
    "    # Create an empty list to store the data\n",
    "    data = []\n",
    "\n",
    "    # Loop through the rows of the table and extract the data\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) > 1:\n",
    "            Hour = cols[0].get_text(strip=True)\n",
    "            Ticker = cols[1].get_text(strip=True)\n",
    "            Imp = cols[2].get_text(strip=True)\n",
    "            Event = cols[3].get_text(strip=True)\n",
    "            Actual = cols[4].get_text(strip=True)\n",
    "            Forecast = cols[5].get_text(strip=True)\n",
    "            Previous = cols[6].get_text(strip=True)\n",
    "            data.append([Hour, Ticker, Imp, Event, Actual, Forecast, Previous])\n",
    "\n",
    "    # Create a dataframe from the scraped data\n",
    "    df = pd.DataFrame(data, columns=[\"Hour\", \"Ticker\", \"Imp\", \"Event\", 'Actual', 'Forecast', 'Previous'])\n",
    "\n",
    "    # Print the dataframe\n",
    "    print(df)\n",
    "\n",
    "    current_date = datetime.date.today()\n",
    "    formatted_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "    ec_EURUSD = df[(df['Ticker'] == 'USD') | (df['Ticker'] == 'EUR')].sort_values('Hour')\n",
    "    ec_EURUSD.to_excel(f\"economic_calendar_{formatted_date}.xlsx\", index=False)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Deep leanring__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install gym    \n",
    "# !pip install keras\n",
    "# !pip install keras-r12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-gpu==1.15.0 tensorflow==1.15.0 stable-baselines gym-anytrading gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2')\n",
    "status = env.observation_space.shape[0]\n",
    "actions = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_timeframe = pd.DataFrame(mt5.copy_rates_from_pos(SYMBOL, mt5.TIMEFRAME_D1, START_POSITION, 1000))[['time', 'open', 'high', 'low', 'close','tick_volume']]\n",
    "daily_timeframe.rename(columns={'tick_volume': 'volume'}, inplace=True)\n",
    "\n",
    "hour_timeframe = pd.DataFrame(mt5.copy_rates_from_pos(self.SYMBOL, mt5.TIMEFRAME_H1, self.START_POSITION, 20000))[['time', 'open', 'high', 'low', 'close','tick_volume']]\n",
    "hour_timeframe.rename(columns={'tick_volume': 'volume'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
